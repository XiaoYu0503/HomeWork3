"""Streamlit Demo for SMS Spam Classifier.
Run:
  streamlit run spam_classifier/streamlit_app.py
"""
from __future__ import annotations
import os
from typing import Optional, List
from collections import Counter

import pandas as pd
import joblib
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from io import BytesIO
from wordcloud import WordCloud
from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_recall_fscore_support

# èˆ‡è¨“ç·´è…³æœ¬è¼¸å‡ºä¸€è‡´ï¼šæ¨¡å‹ä½æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹ models/spam_model.joblib
MODEL_PATH = os.path.join("models", "spam_model.joblib")
DATA_FILE = "sms_spam_no_header.csv"

@st.cache_resource(show_spinner=False)
def load_model() -> Optional[object]:
    if not os.path.exists(MODEL_PATH):
        return None
    return joblib.load(MODEL_PATH)

@st.cache_data(show_spinner=False)
def load_dataset(path: str) -> Optional[pd.DataFrame]:
    if not os.path.exists(path):
        return None
    # è³‡æ–™ç‚ºç„¡è¡¨é ­ä¸”å«å¼•è™Ÿï¼ŒæŒ‡å®š header=None èˆ‡æ¬„ä½åç¨±
    df = pd.read_csv(path, encoding="utf-8", header=None, names=["label", "text"])
    df = df.dropna(subset=["text"]).copy()
    df["text"] = df["text"].astype(str)
    return df

st.set_page_config(page_title="SMS Spam Classifier", page_icon="ğŸ“¨", layout="wide")
st.title("ğŸ“¨ SMS åƒåœ¾ç°¡è¨Šåˆ†é¡å™¨ Demo")

model = load_model()
if model is None:
    st.error("æ¨¡å‹å°šæœªå»ºç«‹ï¼Œè«‹å…ˆåœ¨æ ¹ç›®éŒ„åŸ·è¡Œ: python .\\spam_classifier\\train.py")
    st.stop()

dataset = load_dataset(DATA_FILE)
if dataset is None:
    st.warning("æ‰¾ä¸åˆ°è³‡æ–™æª” sms_spam_no_header.csvï¼Œè³‡æ–™è¦–è¦ºåŒ–åŠŸèƒ½å°‡åœç”¨ã€‚")

with st.sidebar:
    st.header("è¨­å®š")
    show_prob = st.checkbox("é¡¯ç¤ºæ‰€æœ‰é¡åˆ¥æ©Ÿç‡", True)
    batch_limit = st.number_input("æ‰¹æ¬¡é æ¸¬é¡¯ç¤ºç­†æ•¸ä¸Šé™", min_value=5, max_value=200, value=50, step=5)
    st.markdown("---")
    st.subheader("ä»¤ç‰Œæ¨¡å¼")
    token_scope = st.selectbox("è³‡æ–™ç¯„åœ", ["å…¨éƒ¨", "ham", "spam"], index=0)
    token_ngram = st.slider("n-gram é•·åº¦", min_value=1, max_value=2, value=1, step=1)
    token_topk = st.slider("é¡¯ç¤ºå‰ N å€‹å¸¸è¦‹ä»¤ç‰Œ", min_value=10, max_value=100, value=30, step=10)
    top_n_terms = st.slider("Top æ¬Šé‡è©é¡¯ç¤ºæ•¸é‡", min_value=5, max_value=50, value=20, step=5)
    show_wordcloud = st.checkbox("é¡¯ç¤ºè©é›² (WordCloud)", True)
    st.markdown("---")
    st.subheader("æ¨¡å‹æ•ˆèƒ½")
    spam_threshold = st.slider("Spam åˆ¤å®šé–¾å€¼", min_value=0.10, max_value=0.90, value=0.50, step=0.05)
    st.markdown("---")
    st.markdown("**æ¨¡å‹è·¯å¾‘**: ``{}``".format(MODEL_PATH))

# å–®ç­†è¼¸å…¥
st.subheader("å–®ç­†è¨Šæ¯é æ¸¬")
text = st.text_area("è¼¸å…¥ç°¡è¨Šå…§å®¹ï¼š", height=120, placeholder="ä¾‹å¦‚ï¼šFree entry in a weekly cash prize draw")
col_predict, col_clear = st.columns([1,1])
if col_predict.button("ğŸ”® é æ¸¬"):
    if not text.strip():
        st.warning("è«‹è¼¸å…¥è¨Šæ¯å…§å®¹ã€‚")
    else:
        pred = model.predict([text])[0]
        proba = model.predict_proba([text])[0]
        classes = list(model.classes_)
        prob_map = dict(zip(classes, proba))
        is_spam = pred.lower() == "spam"
        color = "#d9534f" if is_spam else "#5cb85c"
        st.markdown(f"<div style='padding:12px;border-radius:6px;background:{color};color:#fff;font-weight:bold;'>åˆ†é¡çµæœï¼š {pred.upper()}</div>", unsafe_allow_html=True)
        if show_prob:
            df_prob = pd.DataFrame({"class": classes, "probability": proba}).sort_values("probability", ascending=False)
            st.table(df_prob)
if col_clear.button("ğŸ§¹ æ¸…é™¤"):
    st.experimental_set_query_params()  # ç®€å–®åˆ·æ–°

st.markdown("---")

# æ‰¹æ¬¡ä¸Šå‚³
st.subheader("æ‰¹æ¬¡é æ¸¬ (CSV ä¸Šå‚³)")
st.caption("æ ¼å¼ï¼šç„¡è¡¨é ­ï¼Œç¬¬ä¸€æ¬„ label(å¯ç•™ç©º)ã€ç¬¬äºŒæ¬„ textã€‚è‹¥å·²æœ‰è¡¨é ­äº¦å¯ä¸Šå‚³ï¼Œç¨‹å¼æœƒå˜—è©¦è¾¨è­˜ã€‚")
uploaded = st.file_uploader("é¸æ“‡ CSV æª”", type=["csv"]) 
if uploaded is not None:
    try:
        # å˜—è©¦è®€å–ï¼šå…ˆå˜—è©¦å«è¡¨é ­ï¼Œä¸è¡Œå‰‡æŒ‡å®šæ¬„ä½
        try:
            df_up = pd.read_csv(uploaded)
            if set(df_up.columns) >= {"label", "text"}:
                pass
            elif df_up.shape[1] >= 2:
                df_up = pd.read_csv(uploaded, header=None, names=["label", "text"])
            else:
                st.error("CSV æ¬„ä½ä¸è¶³ï¼Œéœ€è‡³å°‘ 2 æ¬„ã€‚")
                df_up = None
        except Exception:
            uploaded.seek(0)
            df_up = pd.read_csv(uploaded, header=None, names=["label", "text"])
        if df_up is not None:
            df_up = df_up.dropna(subset=["text"]).head(batch_limit)
            preds = model.predict(df_up["text"].astype(str))
            probas = model.predict_proba(df_up["text"].astype(str))
            classes = list(model.classes_)
            df_result = df_up.copy()
            df_result["pred"] = preds
            # å– spam æ©Ÿç‡ä»¥ä¾¿æ’åºï¼ˆå‡è¨­å­˜åœ¨ spam é¡åˆ¥ï¼‰
            if "spam" in classes:
                spam_index = classes.index("spam")
                df_result["spam_prob"] = [p[spam_index] for p in probas]
            if show_prob:
                # å±•é–‹å„é¡åˆ¥æ©Ÿç‡
                for ci, cname in enumerate(classes):
                    df_result[f"prob_{cname}"] = [p[ci] for p in probas]
            st.write(df_result)
            st.success(f"å®Œæˆ {len(df_result)} ç­†é æ¸¬ã€‚")
    except Exception as e:
        st.error(f"è®€å–æˆ–é æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

st.markdown("---")

# è³‡æ–™é›†è¦–è¦ºåŒ–
st.subheader("è³‡æ–™æ¢ç´¢ / è¦–è¦ºåŒ–")
if dataset is not None:
    with st.expander("åŸå§‹è³‡æ–™å‰ 10 ç­†"):
        st.dataframe(dataset.head(10))

    st.markdown("---")

    # å„€è¡¨æ¿åˆ†é 
    tabs = st.tabs(["è³‡æ–™åˆ†ä½ˆ", "ä»¤ç‰Œæ¨¡å¼", "æ¨¡å‹æ•ˆèƒ½"])

    # è³‡æ–™åˆ†ä½ˆ
    with tabs[0]:
        st.subheader("è³‡æ–™åˆ†ä½ˆ")
        if dataset is not None:
            with st.expander("åŸå§‹è³‡æ–™å‰ 10 ç­†"):
                st.dataframe(dataset.head(10))

            col_a, col_b = st.columns([1,2])
            label_counts = dataset["label"].value_counts()
            col_a.metric("ç¸½ç­†æ•¸", f"{len(dataset):,}")
            col_a.write(label_counts)

            dataset["length"] = dataset["text"].str.len()
            fig_len, ax_len = plt.subplots(figsize=(6,3))
            ax_len.hist(dataset["length"], bins=40, color="#4e79a7", alpha=0.7, label="All")
            # é¡åˆ¥å°æ¯”ç›´æ–¹åœ–
            try:
                ax_len.hist(dataset.loc[dataset.label.str.lower()=="ham","length"], bins=40, alpha=0.5, label="ham")
                ax_len.hist(dataset.loc[dataset.label.str.lower()=="spam","length"], bins=40, alpha=0.5, label="spam")
                ax_len.legend()
            except Exception:
                pass
            ax_len.set_title("è¨Šæ¯é•·åº¦ç›´æ–¹åœ–")
            ax_len.set_xlabel("å­—å…ƒæ•¸")
            ax_len.set_ylabel("é »ç‡")
            col_b.pyplot(fig_len, clear_figure=True)
        else:
            st.info("è³‡æ–™æª”ç¼ºå¤±ï¼Œåƒ…èƒ½ä½¿ç”¨é æ¸¬åŠŸèƒ½ã€‚")

    # ä»¤ç‰Œæ¨¡å¼
    with tabs[1]:
        st.subheader("ä»¤ç‰Œæ¨¡å¼ï¼ˆä¾è³‡æ–™èˆ‡å‘é‡å™¨ï¼‰")
        try:
            if dataset is not None and hasattr(model, "named_steps") and "tfidf" in model.named_steps:
                vect = model.named_steps["tfidf"]
                analyzer = vect.build_analyzer()
                # ç¯©é¸è³‡æ–™ç¯„åœ
                if token_scope == "ham":
                    texts = dataset.loc[dataset.label.str.lower()=="ham","text"].astype(str).tolist()
                elif token_scope == "spam":
                    texts = dataset.loc[dataset.label.str.lower()=="spam","text"].astype(str).tolist()
                else:
                    texts = dataset["text"].astype(str).tolist()
                counter = Counter()
                for t in texts:
                    toks = analyzer(t)
                    # ç¯©é¸ n-gram é•·åº¦
                    for tok in toks:
                        if (tok.count(" ")+1) == token_ngram:
                            counter[tok] += 1
                common = counter.most_common(token_topk)
                df_tok = pd.DataFrame(common, columns=["token", "count"])
                st.caption(f"Top {token_topk} ä»¤ç‰Œï¼ˆn={token_ngram}, ç¯„åœ={token_scope}ï¼‰")
                st.table(df_tok)
                # å¯é¸ï¼šé»é¸ä¸€å€‹ token é¡¯ç¤ºç¯„ä¾‹å¥
                if len(df_tok):
                    picked = st.selectbox("æŸ¥çœ‹åŒ…å«æ­¤ä»¤ç‰Œçš„ç¯„ä¾‹å¥ï¼š", ["(ä¸é¸)"] + df_tok["token"].head(20).tolist())
                    if picked and picked != "(ä¸é¸)":
                        examples = [s for s in texts if picked in s][:5]
                        for ex in examples:
                            st.write("â€¢ ", ex)
            else:
                st.info("ç¼ºå°‘è³‡æ–™æˆ–å‘é‡å™¨ï¼Œç„¡æ³•é¡¯ç¤ºä»¤ç‰Œæ¨¡å¼ã€‚")
        except Exception as e:
            st.info(f"ä»¤ç‰Œæ¨¡å¼è¨ˆç®—å¤±æ•—ï¼š{e}")

    # æ¨¡å‹æ•ˆèƒ½
    with tabs[2]:
        st.subheader("æ¨¡å‹æ•ˆèƒ½ï¼ˆæ•´ä»½è³‡æ–™é›†é‡è·‘æ¨è«–ï¼‰")
        try:
            if dataset is not None:
                y_true = dataset["label"].astype(str)
                # ä½¿ç”¨æ©Ÿç‡ + é–¾å€¼ç”¢ç”Ÿé æ¸¬
                if hasattr(model, "predict_proba"):
                    proba_full = model.predict_proba(dataset["text"].astype(str))
                    classes = list(model.classes_)
                    if "spam" in classes:
                        spam_index = classes.index("spam")
                        spam_scores = proba_full[:, spam_index]
                        y_pred_thr = np.where(spam_scores >= spam_threshold, "spam", "ham")
                    else:
                        # å¾Œå‚™ï¼šç›´æ¥ä½¿ç”¨ predict
                        y_pred_thr = model.predict(dataset["text"].astype(str))
                        spam_scores = None
                else:
                    y_pred_thr = model.predict(dataset["text"].astype(str))
                    spam_scores = None

                # æŒ‡æ¨™
                acc = accuracy_score(y_true, y_pred_thr)
                prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred_thr, labels=["ham","spam"], average=None)
                col_m1, col_m2, col_m3 = st.columns(3)
                col_m1.metric("Accuracy", f"{acc:.4f}")
                # é¡¯ç¤º spam é€™ä¸€é¡çš„ P/R/F1
                try:
                    spam_idx = ["ham","spam"].index("spam")
                    col_m2.metric("Precision (spam)", f"{prec[spam_idx]:.4f}")
                    col_m3.metric("Recall (spam)", f"{rec[spam_idx]:.4f}")
                    st.caption(f"F1 (spam) = {f1[spam_idx]:.4f}")
                except Exception:
                    pass

                # æ··æ·†çŸ©é™£
                cm = confusion_matrix(y_true, y_pred_thr, labels=["ham","spam"])
                fig_cm, ax_cm = plt.subplots(figsize=(4,3))
                im = ax_cm.imshow(cm, cmap="Blues")
                ax_cm.set_xticks([0,1]); ax_cm.set_xticklabels(["ham","spam"])
                ax_cm.set_yticks([0,1]); ax_cm.set_yticklabels(["ham","spam"])
                ax_cm.set_xlabel("Predicted"); ax_cm.set_ylabel("Actual")
                for (i,j), v in np.ndenumerate(cm):
                    ax_cm.text(j, i, str(v), ha="center", va="center", color="black")
                ax_cm.set_title(f"Confusion Matrix (thresh={spam_threshold:.2f})")
                st.pyplot(fig_cm, clear_figure=True)

                # ROCï¼ˆèˆ‡é–¾å€¼ç„¡é—œï¼‰
                if spam_scores is not None:
                    y_bin = (y_true.str.lower()=="spam").astype(int)
                    fpr, tpr, _ = roc_curve(y_bin, spam_scores)
                    roc_auc = auc(fpr, tpr)
                    fig_roc, ax_roc = plt.subplots(figsize=(4,3))
                    ax_roc.plot(fpr, tpr, label=f"ROC AUC={roc_auc:.3f}")
                    ax_roc.plot([0,1],[0,1], linestyle="--", color="gray")
                    ax_roc.set_xlabel("FPR")
                    ax_roc.set_ylabel("TPR")
                    ax_roc.set_title("ROC Curve (spam as positive)")
                    ax_roc.legend(loc="lower right")
                    st.pyplot(fig_roc, clear_figure=True)

                # åŒ¯å‡º
                st.markdown("### åŒ¯å‡ºé æ¸¬çµæœ")
                full_df = dataset.copy()
                full_df["pred"] = y_pred_thr
                if spam_scores is not None:
                    full_df["spam_prob"] = spam_scores
                csv_bytes = full_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ä¸‹è¼‰å®Œæ•´é æ¸¬çµæœ CSV", data=csv_bytes, file_name="spam_predictions.csv", mime="text/csv")
            else:
                st.info("è³‡æ–™æª”ç¼ºå¤±ï¼Œç„¡æ³•è¨ˆç®—æ•ˆèƒ½ã€‚")
        except Exception as e:
            st.info(f"æ•ˆèƒ½è¨ˆç®—å¤±æ•—ï¼š{e}")

# èªªæ˜å€å¡Š
st.markdown("---")
with st.expander("èªªæ˜ / Help"):
    st.markdown(
        """
        **ä½¿ç”¨èªªæ˜**
        - å–®ç­†è¼¸å…¥å€è¼¸å…¥è¨Šæ¯å¾ŒæŒ‰ä¸‹ã€é æ¸¬ã€ã€‚
        - æ‰¹æ¬¡ä¸Šå‚³æ”¯æ´ CSVï¼Œå‰å…©æ¬„è¦–ç‚º label èˆ‡ textï¼›label å¯ç‚ºç©ºç”¨æ–¼æ¨è«–ã€‚
        - è‹¥å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œï¼š`python .\\spam_classifier\\train.py`ã€‚

        **æ”¹é€²å»ºè­°**
        - å¯å¢åŠ è³‡æ–™æ¸…ç†ï¼ˆURLã€è¡¨æƒ…ç¬¦è™Ÿæ­£è¦åŒ–ï¼‰ã€‚
        - å¯æ›¿æ›æ¨¡å‹ç‚º SVCã€Naive Bayes æˆ–æ·±åº¦å­¸ç¿’ã€‚
        - å¯åŠ å…¥æ··æ·†çŸ©é™£èˆ‡ ROC æ›²ç·šè¦–è¦ºåŒ–ã€‚
        """
    )
